{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image captioning example.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps_3f1mwLtjS"
      },
      "source": [
        "# Unzip data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_bRkb7hLxF4",
        "outputId": "837d0ac7-60fb-49fd-d6f5-9d7d999e8322"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)\n",
        "data_folder = r'/content/drive/My Drive/data/'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gn-c0m-DL0Ar",
        "outputId": "eb76043c-7c9c-4029-bc9d-bb6e22c37e3b"
      },
      "source": [
        "!unzip '/content/drive/My Drive/data/flickr1k.zip' -d '/content/drive/My Drive/data/'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/My Drive/data/flickr1k.zip\n",
            "replace /content/drive/My Drive/data/flickr1k/captions.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pe6czCFkMGau"
      },
      "source": [
        "# Module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdqSi5pJMJ4b"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import spacy\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vAPUTaFjMShr"
      },
      "source": [
        "spacy_eng = spacy.load('en')\n",
        "\n",
        "\n",
        "class Vocabulary:\n",
        "  def __init__(self, freq_threshold):\n",
        "    self.itos = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
        "    self.stoi = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
        "    self.freq_threshold = freq_threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "\n",
        "  @staticmethod\n",
        "  def tokenizer_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
        "\n",
        "  def build_vocabulary(self, sentence_list):\n",
        "    frequencies = {}\n",
        "    idx = 4\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "      for word in self.tokenizer_eng(sentence):\n",
        "        if word not in frequencies:\n",
        "          frequencies[word] = 1\n",
        "        else:\n",
        "          frequencies[word] += 1\n",
        "\n",
        "        if frequencies[word] == self.freq_threshold:\n",
        "          self.stoi[word] = idx\n",
        "          self.itos[idx] = word\n",
        "          idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "    return [\n",
        "              self.stoi[token] if token in self.stoi else self.stoi['<UNK>'] \n",
        "              for token in tokenized_text\n",
        "    ]\n",
        "\n",
        "\n",
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self, root_dir, captions_file, transform, freq_threshold=5):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(captions_file)\n",
        "    self.transform = transform\n",
        "\n",
        "    self.imgs = self.df['image']\n",
        "    self.captions = self.df['caption']\n",
        "\n",
        "    self.vocab = Vocabulary(freq_threshold)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "    img = Image.open(os.path.join(self.root_dir, img_id)).convert('RGB')\n",
        "\n",
        "    img = self.transform(img)\n",
        "    \n",
        "    numaricalized_caption = [self.vocab.stoi['<SOS>']]\n",
        "    numaricalized_caption += self.vocab.numericalize(caption)\n",
        "    numaricalized_caption.append(self.vocab.stoi['<EOS>'])\n",
        "\n",
        "    return img, torch.tensor(numaricalized_caption)\n",
        "\n",
        "\n",
        "class FlickrTest(Dataset):\n",
        "  def __init__(self, root_dir, captions_file, transform, freq_threshold=5):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(captions_file)\n",
        "    unique_ids = self.df['caption'].unique()\n",
        "    test_ids = unique_ids[np.random.choice(unique_ids.shape[0], 1, replace=False)]\n",
        "    self.df = self.df[self.df['caption'].isin(test_ids)]\n",
        "    self.df = self.df.reset_index()\n",
        "\n",
        "    self.transform = transform\n",
        "\n",
        "    self.imgs = self.df['image']\n",
        "    self.captions = self.df['caption']\n",
        "    print(self.captions)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "    img = Image.open(os.path.join(self.root_dir, img_id)).convert('RGB')\n",
        "\n",
        "    img_transformed = self.transform(img)\n",
        "\n",
        "    return np.array(img), img_transformed\n",
        "\n",
        "\n",
        "class MyCollate:\n",
        "  def __init__(self, pad_idx):\n",
        "    self.pad_idx = pad_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    imgs = torch.cat(imgs, dim=0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "    return imgs, targets"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-Ht04H6RI_8"
      },
      "source": [
        "class EncoderCNN(nn.Module):\n",
        "  def __init__(self, embed_size, train=False):\n",
        "    super(EncoderCNN, self).__init__()\n",
        "    self.train = train\n",
        "    self.resnet = models.resnet18(pretrained=True)\n",
        "    self.resnet.fc = nn.Linear(self.resnet.fc.in_features, embed_size)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, images):\n",
        "    features = self.resnet(images)\n",
        "\n",
        "    for name, param in self.resnet.named_parameters():\n",
        "      if 'fc.weight' in name or 'fc.bias' in name:\n",
        "        param.requires_grad = True\n",
        "      else:\n",
        "        param.requires_grad = self.train\n",
        "\n",
        "    return self.dropout(self.relu(features))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNI_Yc-tR5X8"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(DecoderRNN, self).__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "    self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "    self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "  def forward(self, features, captions):\n",
        "    embeddings = self.dropout(self.embedding(captions))\n",
        "    embeddings = torch.cat((features.unsqueeze(0), embeddings), dim=0)\n",
        "    hiddens, _ = self.lstm(embeddings)\n",
        "    outputs = self.linear(hiddens)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNJ4fDkfXTuf"
      },
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "  def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "    super(CNNtoRNN, self).__init__()\n",
        "    self.encoder = EncoderCNN(embed_size)\n",
        "    self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "  def forward(self, images, captions):\n",
        "    features = self.encoder(images)\n",
        "    outputs = self.decoder(features, captions)\n",
        "    return outputs"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPuwhj_NRKhH"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_r_JT4XRM14"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "  [\n",
        "   transforms.Resize((256,256)),\n",
        "   transforms.CenterCrop(224),\n",
        "   transforms.ToTensor(),\n",
        "   transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "  ]\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIyOtmrhQT-f",
        "outputId": "cf202d0a-71cc-46e3-a376-bde96442df99"
      },
      "source": [
        "trainset = FlickrDataset(root_dir='/content/drive/My Drive/data/flickr1k/images', \n",
        "                         captions_file='/content/drive/My Drive/data/flickr1k/captions.csv', \n",
        "                         transform=transform)\n",
        "pad_idx = trainset.vocab.stoi['<PAD>']\n",
        "train_loader = DataLoader(\n",
        "    dataset=trainset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
        "  )\n",
        "\n",
        "testset = FlickrTest(root_dir='/content/drive/My Drive/data/flickr1k/images', \n",
        "                     captions_file='/content/drive/My Drive/data/flickr1k/captions.csv', \n",
        "                     transform=transform)\n",
        "test_loader = DataLoader(\n",
        "    dataset=testset,\n",
        "    batch_size=32,\n",
        "    shuffle=False\n",
        "  )"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    a dog jumping over a small wall at a beach nea...\n",
            "Name: caption, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OZRXGYvCPWv2"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  \n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = len(trainset.vocab)\n",
        "num_layers = 1\n",
        "learning_rate = 3e-4\n",
        "num_epochs = 10"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZdCEjcpSG-8",
        "outputId": "f3443f03-9cac-4a52-e288-381e71d586c7"
      },
      "source": [
        "losses = list()\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=trainset.vocab.stoi['<PAD>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "    \n",
        "    for i_step, (img, caption) in enumerate(train_loader):\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "        img = img.to(device)\n",
        "        captions_target = caption[1:].to(device)\n",
        "        captions_train = caption[:-1].to(device)\n",
        "\n",
        "        outputs = model(img, captions_train)\n",
        "\n",
        "        loss = criterion(outputs[1:].view(-1, vocab_size), captions_target.contiguous().view(-1))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "        stats = 'Epoch [%d/%d], Step [%d], Loss: %.4f' % (epoch, num_epochs, i_step, loss.item())\n",
        "\n",
        "        print('\\r' + stats, end='')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Step [156], Loss: 2.7136"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UG-NtbuhgkBu"
      },
      "source": [
        "# testset = FlickrTest(root_dir='/content/drive/My Drive/data/flickr1k/images', \n",
        "#                      captions_file='/content/drive/My Drive/data/flickr1k/captions.csv', \n",
        "#                      transform=transform)\n",
        "# test_loader = DataLoader(\n",
        "#     dataset=testset,\n",
        "#     batch_size=32,\n",
        "#     shuffle=False\n",
        "#   )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDZWFCtVg_UW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bajXciEPchR_"
      },
      "source": [
        "with torch.no_grad():\n",
        "  image_caption = []\n",
        "  for (img, img_transformed) in test_loader:\n",
        "    img_transformed = img_transformed.to(device)\n",
        "\n",
        "    x = model.encoder(img_transformed).unsqueeze(0)\n",
        "    states = None\n",
        "\n",
        "    for _ in range(20):\n",
        "      hiddens, states = model.decoder.lstm(x, states)\n",
        "      output = model.decoder.linear(hiddens.squeeze(0))\n",
        "      predicted = output.argmax(1)\n",
        "      \n",
        "      image_caption.append([x.item() for x in predicted])\n",
        "      x = model.decoder.embedding(predicted).unsqueeze(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4NZAeLRlhC6g",
        "outputId": "a2f83219-4c3c-4a1b-d7ac-71c8b1aa7829"
      },
      "source": [
        "[trainset.vocab.itos[idx] for idx in np.array(image_caption)[:,0].tolist()]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'man',\n",
              " 'in',\n",
              " 'a',\n",
              " 'red',\n",
              " 'jacket',\n",
              " 'is',\n",
              " 'standing',\n",
              " 'on',\n",
              " 'a',\n",
              " '<UNK>',\n",
              " '.',\n",
              " '<EOS>',\n",
              " 'a',\n",
              " '<UNK>',\n",
              " '.',\n",
              " '<EOS>',\n",
              " '<EOS>',\n",
              " '.',\n",
              " '<EOS>']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kWyJhVGliUke"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}